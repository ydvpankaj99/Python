{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07.DataSetsPySpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ydvpankaj99/Python/blob/master/07_DataSetsPySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD"
      ],
      "metadata": {
        "id": "XBluYiZlyknD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK00KAACwIHt"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.master(\"local[1]\").appName(\"abc.com\").getOrCreate()\n",
        "data =list(range(1,13))\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgM4XK-rysmF",
        "outputId": "6389a9d2-67a0-4741-b187-9f96aef7d587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1=spark.sparkContext.parallelize(data)\n",
        "print(\"RDD1 Partitions are: \",rdd1.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSERW05FzC8F",
        "outputId": "60e5ba0c-f412-462f-ed58-939db6239c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD1 Partitions are:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2=rdd1.repartition(4)\n",
        "result=\"RePartition Count is {0}\".format(rdd2.getNumPartitions())\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqMMb7bMzzh-",
        "outputId": "fba37af4-e10d-456c-b1a7-8ad7eb6c30fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RePartition Count is 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd4=spark.sparkContext.wholeTextFiles(\"/content/emp.txt\")\n",
        "rdd4=spark.sparkContext.emptyRDD\n",
        "rdd4=spark.sparkContext.parallelize([],10)\n",
        "print(rdd4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAuVvErO0I5S",
        "outputId": "3851c291-bca8-4d6c-b2ed-95e1be837c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[12] at readRDDFromFile at PythonRDD.scala:274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=spark.sparkContext.textFile(\"/content/emp.txt\")\n",
        "rdd2=rdd.flatMap(lambda x:x.split(\" \"))\n",
        "rdd2.saveAsTextFile(\"/content/outputs/\")"
      ],
      "metadata": {
        "id": "e1ERJNUe7GDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(rdd2.saveAsTextFile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjT04bZq78t3",
        "outputId": "c6a28171-00eb-4348-b188-11d604ba71d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method saveAsTextFile in module pyspark.rdd:\n",
            "\n",
            "saveAsTextFile(path: str, compressionCodecClass: Union[str, NoneType] = None) -> None method of pyspark.rdd.PipelinedRDD instance\n",
            "    Save this RDD as a text file, using string representations of elements.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    path : str\n",
            "        path to text file\n",
            "    compressionCodecClass : str, optional\n",
            "        fully qualified classname of the compression codec class\n",
            "        i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> from tempfile import NamedTemporaryFile\n",
            "    >>> tempFile = NamedTemporaryFile(delete=True)\n",
            "    >>> tempFile.close()\n",
            "    >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
            "    >>> from fileinput import input\n",
            "    >>> from glob import glob\n",
            "    >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
            "    '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
            "    \n",
            "    Empty lines are tolerated when saving to text files.\n",
            "    \n",
            "    >>> from tempfile import NamedTemporaryFile\n",
            "    >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
            "    >>> tempFile2.close()\n",
            "    >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
            "    >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
            "    '\\n\\n\\nbar\\nfoo\\n'\n",
            "    \n",
            "    Using compressionCodecClass\n",
            "    \n",
            "    >>> from tempfile import NamedTemporaryFile\n",
            "    >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
            "    >>> tempFile3.close()\n",
            "    >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
            "    >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
            "    >>> from fileinput import input, hook_compressed\n",
            "    >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
            "    >>> ''.join([r.decode('utf-8') if isinstance(r, bytes) else r for r in result])\n",
            "    'bar\\nfoo\\n'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[\"welcome to java\",\"pyspark is good\",\"ML can be used for Intelligence\"]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "for element in rdd.collect():\n",
        "  print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrEy-hl18JDU",
        "outputId": "37b321e1-b8e6-4713-df24-9b95afe34a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "welcome to java\n",
            "pyspark is good\n",
            "ML can be used for Intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=lambda x:x.split(\" \")\n",
        "rdd2=rdd.flatMap(f)\n",
        "for element in rdd2.collect():\n",
        "  print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu5iz-r59bgI",
        "outputId": "9c627f81-d913-4fb8-808e-cb8ca04e4eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "welcome\n",
            "to\n",
            "java\n",
            "pyspark\n",
            "is\n",
            "good\n",
            "ML\n",
            "can\n",
            "be\n",
            "used\n",
            "for\n",
            "Intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4xmrYirK96V5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}